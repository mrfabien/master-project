{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of a simple regression model with the mean of each variable in datasets_3h folder\n",
    "# and the mean of the target variable in the training set\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x169671690>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_storm = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/instantaneous_10m_wind_gust/storm_1/max_1_0.csv')\n",
    "one_storm = np.asarray(one_storm['0'])\n",
    "plt.plot(one_storm, label='True', lw=0, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# test 2\\n\\n# open each csv file and read it into a pandas dataframe\\nname_of_variables = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/variable_list_24_v2_1.csv')\\nname_of_variables = name_of_variables['variable'].tolist()\\n\\n# Create a new list of variables\\nnew_name_of_variables = []\\nfor variable in name_of_variables:\\n    if variable == 'geopotential':\\n        new_name_of_variables.extend(['geopotential_500', 'geopotential_1000'])\\n    else:\\n        new_name_of_variables.append(variable)\\n\\n# Iterate over the new list of variables\\nfor i in range(0, len(new_name_of_variables)):\\n\\n    print(new_name_of_variables[i])\\n\\n    locals()[f'max_{new_name_of_variables[i]}'] = pd.DataFrame()\\n    locals()[f'min_{new_name_of_variables[i]}'] = pd.DataFrame()\\n    locals()[f'mean_{new_name_of_variables[i]}'] = pd.DataFrame()\\n    locals()[f'sigma_{new_name_of_variables[i]}'] = pd.DataFrame()\\n\\n    for j in range (1,96+1):\\n        if 'geopotential' in new_name_of_variables[i]:\\n            for k in [500, 1000]:\\n                df_max_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/geopotential/storm_' + str(j) + '/max_'+ str(j) + '_' + str(k) + '.csv')\\n                df_min_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/geopotential/storm_' + str(j) + '/min_'+ str(j) + '_' + str(k) + '.csv')\\n                df_mean_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/geopotential/storm_' + str(j) + '/mean_'+ str(j) + '_' + str(k) + '.csv')\\n                df_sigma_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/geopotential/storm_' + str(j) + '/std_'+ str(j) + '_' + str(k) + '.csv')\\n\\n        else:\\n            df_max_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/' + new_name_of_variables[i] + '/storm_' + str(j) + '/max_'+ str(j) + '_0.csv')\\n            df_min_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/' + new_name_of_variables[i] + '/storm_' + str(j) + '/min_'+ str(j) + '_0.csv')\\n            df_mean_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/' + new_name_of_variables[i] + '/storm_' + str(j) + '/mean_'+ str(j) + '_0.csv')\\n            df_sigma_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/' + new_name_of_variables[i] + '/storm_' + str(j) + '/std_'+ str(j) + '_0.csv')\\n\\n        df_max_temp = df_max_temp.drop(columns = ['Unnamed: 0'])\\n        df_min_temp = df_min_temp.drop(columns = ['Unnamed: 0'])\\n        df_mean_temp = df_mean_temp.drop(columns = ['Unnamed: 0'])\\n        df_sigma_temp = df_sigma_temp.drop(columns = ['Unnamed: 0'])\\n\\n        locals()[f'max_{new_name_of_variables[i]}'] = pd.concat([locals()[f'max_{new_name_of_variables[i]}'], df_max_temp], axis=0)\\n        locals()[f'min_{new_name_of_variables[i]}'] = pd.concat([locals()[f'min_{new_name_of_variables[i]}'], df_min_temp], axis=0)\\n        locals()[f'mean_{new_name_of_variables[i]}'] = pd.concat([locals()[f'mean_{new_name_of_variables[i]}'], df_mean_temp], axis=0)\\n        locals()[f'sigma_{new_name_of_variables[i]}'] = pd.concat([locals()[f'sigma_{new_name_of_variables[i]}'], df_sigma_temp], axis=0)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# test 2\n",
    "\n",
    "# open each csv file and read it into a pandas dataframe\n",
    "name_of_variables = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/variable_list_24_v2_1.csv')\n",
    "name_of_variables = name_of_variables['variable'].tolist()\n",
    "\n",
    "# Create a new list of variables\n",
    "new_name_of_variables = []\n",
    "for variable in name_of_variables:\n",
    "    if variable == 'geopotential':\n",
    "        new_name_of_variables.extend(['geopotential_500', 'geopotential_1000'])\n",
    "    else:\n",
    "        new_name_of_variables.append(variable)\n",
    "\n",
    "# Iterate over the new list of variables\n",
    "for i in range(0, len(new_name_of_variables)):\n",
    "\n",
    "    print(new_name_of_variables[i])\n",
    "\n",
    "    locals()[f'max_{new_name_of_variables[i]}'] = pd.DataFrame()\n",
    "    locals()[f'min_{new_name_of_variables[i]}'] = pd.DataFrame()\n",
    "    locals()[f'mean_{new_name_of_variables[i]}'] = pd.DataFrame()\n",
    "    locals()[f'sigma_{new_name_of_variables[i]}'] = pd.DataFrame()\n",
    "\n",
    "    for j in range (1,96+1):\n",
    "        if 'geopotential' in new_name_of_variables[i]:\n",
    "            for k in [500, 1000]:\n",
    "                df_max_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/geopotential/storm_' + str(j) + '/max_'+ str(j) + '_' + str(k) + '.csv')\n",
    "                df_min_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/geopotential/storm_' + str(j) + '/min_'+ str(j) + '_' + str(k) + '.csv')\n",
    "                df_mean_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/geopotential/storm_' + str(j) + '/mean_'+ str(j) + '_' + str(k) + '.csv')\n",
    "                df_sigma_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/geopotential/storm_' + str(j) + '/std_'+ str(j) + '_' + str(k) + '.csv')\n",
    "\n",
    "        else:\n",
    "            df_max_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/' + new_name_of_variables[i] + '/storm_' + str(j) + '/max_'+ str(j) + '_0.csv')\n",
    "            df_min_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/' + new_name_of_variables[i] + '/storm_' + str(j) + '/min_'+ str(j) + '_0.csv')\n",
    "            df_mean_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/' + new_name_of_variables[i] + '/storm_' + str(j) + '/mean_'+ str(j) + '_0.csv')\n",
    "            df_sigma_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/' + new_name_of_variables[i] + '/storm_' + str(j) + '/std_'+ str(j) + '_0.csv')\n",
    "\n",
    "        df_max_temp = df_max_temp.drop(columns = ['Unnamed: 0'])\n",
    "        df_min_temp = df_min_temp.drop(columns = ['Unnamed: 0'])\n",
    "        df_mean_temp = df_mean_temp.drop(columns = ['Unnamed: 0'])\n",
    "        df_sigma_temp = df_sigma_temp.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "        locals()[f'max_{new_name_of_variables[i]}'] = pd.concat([locals()[f'max_{new_name_of_variables[i]}'], df_max_temp], axis=0)\n",
    "        locals()[f'min_{new_name_of_variables[i]}'] = pd.concat([locals()[f'min_{new_name_of_variables[i]}'], df_min_temp], axis=0)\n",
    "        locals()[f'mean_{new_name_of_variables[i]}'] = pd.concat([locals()[f'mean_{new_name_of_variables[i]}'], df_mean_temp], axis=0)\n",
    "        locals()[f'sigma_{new_name_of_variables[i]}'] = pd.concat([locals()[f'sigma_{new_name_of_variables[i]}'], df_sigma_temp], axis=0)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10m_u_component_of_wind\n",
      "10m_v_component_of_wind\n",
      "2m_dewpoint_temperature\n",
      "2m_temperature\n",
      "cloud_base_height\n",
      "convective_available_potential_energy\n",
      "convective_inhibition\n",
      "convective_precipitation\n",
      "convective_rain_rate\n",
      "convective_snowfall\n",
      "geopotential_500\n",
      "geopotential_1000\n",
      "high_cloud_cover\n",
      "instantaneous_10m_wind_gust\n",
      "k_index\n",
      "large_scale_precipitation\n",
      "large_scale_snowfall\n",
      "mean_large_scale_precipitation_rate\n",
      "mean_top_net_long_wave_radiation_flux\n",
      "mean_top_net_short_wave_radiation_flux\n",
      "mean_total_precipitation_rate\n",
      "mean_sea_level_pressure\n",
      "mean_surface_latent_heat_flux\n",
      "mean_surface_net_long_wave_radiation_flux\n",
      "mean_surface_net_short_wave_radiation_flux\n",
      "mean_vertically_integrated_moisture_divergence\n",
      "surface_pressure\n",
      "total_precipitation\n",
      "total_totals_index\n"
     ]
    }
   ],
   "source": [
    "# to separate each storm into one column : \n",
    "\n",
    "# open each csv file and read it into a pandas dataframe\n",
    "name_of_variables = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/variable_list_24_v2_1.csv')\n",
    "name_of_variables = name_of_variables['variable'].tolist()\n",
    "\n",
    "# Create a new list of variables\n",
    "new_name_of_variables = []\n",
    "for variable in name_of_variables:\n",
    "    if variable == 'geopotential':\n",
    "        new_name_of_variables.extend(['geopotential_500', 'geopotential_1000'])\n",
    "    else:\n",
    "        new_name_of_variables.append(variable)\n",
    "\n",
    "# Iterate over the new list of variables\n",
    "for i in range(0, len(new_name_of_variables)):\n",
    "\n",
    "    print(new_name_of_variables[i])\n",
    "\n",
    "    locals()[f'max_{new_name_of_variables[i]}'] = pd.DataFrame()\n",
    "    locals()[f'min_{new_name_of_variables[i]}'] = pd.DataFrame()\n",
    "    locals()[f'mean_{new_name_of_variables[i]}'] = pd.DataFrame()\n",
    "    locals()[f'sigma_{new_name_of_variables[i]}'] = pd.DataFrame()\n",
    "\n",
    "    for j in range (1,96+1):\n",
    "        if 'geopotential' in new_name_of_variables[i]:\n",
    "            continue\n",
    "            for k in [500, 1000]:\n",
    "                df_max_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/geopotential/storm_' + str(j) + '/max_'+ str(j) + '_' + str(k) + '.csv')\n",
    "                df_min_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/geopotential/storm_' + str(j) + '/min_'+ str(j) + '_' + str(k) + '.csv')\n",
    "                df_mean_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/geopotential/storm_' + str(j) + '/mean_'+ str(j) + '_' + str(k) + '.csv')\n",
    "                df_sigma_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/geopotential/storm_' + str(j) + '/std_'+ str(j) + '_' + str(k) + '.csv')\n",
    "\n",
    "        else:\n",
    "            df_max_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/' + new_name_of_variables[i] + '/storm_' + str(j) + '/max_'+ str(j) + '_0.csv')\n",
    "            df_min_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/' + new_name_of_variables[i] + '/storm_' + str(j) + '/min_'+ str(j) + '_0.csv')\n",
    "            df_mean_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/' + new_name_of_variables[i] + '/storm_' + str(j) + '/mean_'+ str(j) + '_0.csv')\n",
    "            df_sigma_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/' + new_name_of_variables[i] + '/storm_' + str(j) + '/std_'+ str(j) + '_0.csv')\n",
    "\n",
    "        df_max_temp = df_max_temp.drop(columns = ['Unnamed: 0'])\n",
    "        df_min_temp = df_min_temp.drop(columns = ['Unnamed: 0'])\n",
    "        df_mean_temp = df_mean_temp.drop(columns = ['Unnamed: 0'])\n",
    "        df_sigma_temp = df_sigma_temp.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "        locals()[f'max_{new_name_of_variables[i]}'][f'storm_{j}'] = df_max_temp['0']\n",
    "        locals()[f'min_{new_name_of_variables[i]}'][f'storm_{j}'] = df_min_temp['0']# = pd.concat([locals()[f'min_{new_name_of_variables[i]}'], df_min_temp], axis=0)\n",
    "        locals()[f'mean_{new_name_of_variables[i]}'][f'storm_{j}'] = df_mean_temp['0'] #= pd.concat([locals()[f'mean_{new_name_of_variables[i]}'], df_mean_temp], axis=0)\n",
    "        locals()[f'sigma_{new_name_of_variables[i]}'][f'storm_{j}'] = df_sigma_temp['0'] #= pd.concat([locals()[f'sigma_{new_name_of_variables[i]}'], df_sigma_temp], axis=0)\n",
    "\n",
    "for k in [500, 1000]:\n",
    "    locals()[f'max_geopotential_{k}'] = pd.DataFrame()\n",
    "    locals()[f'min_geopotential_{k}'] = pd.DataFrame()\n",
    "    locals()[f'mean_geopotential_{k}'] = pd.DataFrame()\n",
    "    locals()[f'sigma_geopotential_{k}'] = pd.DataFrame()\n",
    "    for j in range (1,96+1):\n",
    "        df_max_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/geopotential/storm_' + str(j) + '/max_'+ str(j) + '_' + str(k) + '.csv')\n",
    "        df_min_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/geopotential/storm_' + str(j) + '/min_'+ str(j) + '_' + str(k) + '.csv')\n",
    "        df_mean_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/geopotential/storm_' + str(j) + '/mean_'+ str(j) + '_' + str(k) + '.csv')\n",
    "        df_sigma_temp = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h/geopotential/storm_' + str(j) + '/std_'+ str(j) + '_' + str(k) + '.csv')\n",
    "\n",
    "        df_max_temp = df_max_temp.drop(columns = ['Unnamed: 0'])\n",
    "        df_min_temp = df_min_temp.drop(columns = ['Unnamed: 0'])\n",
    "        df_mean_temp = df_mean_temp.drop(columns = ['Unnamed: 0'])\n",
    "        df_sigma_temp = df_sigma_temp.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "        locals()[f'max_geopotential_{k}'][f'storm_{j}'] = df_max_temp['0']\n",
    "        locals()[f'min_geopotential_{k}'][f'storm_{j}'] = df_min_temp['0']# = pd.concat([locals()[f'min_{new_name_of_variables[i]}'], df_min_temp], axis=0)\n",
    "        locals()[f'mean_geopotential_{k}'][f'storm_{j}'] = df_mean_temp['0'] #= pd.concat([locals()[f'mean_{new_name_of_variables[i]}'], df_mean_temp], axis=0)\n",
    "        locals()[f'sigma_geopotential_{k}'][f'storm_{j}'] = df_sigma_temp['0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def process_storm_data(variable_list_path, data_folder_path):\\n    # Read the list of variables\\n    name_of_variables = pd.read_csv(variable_list_path)\\n    name_of_variables = name_of_variables['variable'].tolist()\\n\\n    # Create a new list of variables\\n    new_name_of_variables = []\\n    for variable in name_of_variables:\\n        if variable == 'geopotential':\\n            new_name_of_variables.extend(['geopotential_500', 'geopotential_1000'])\\n        else:\\n            new_name_of_variables.append(variable)\\n\\n    # Create dictionaries to hold the DataFrames\\n    max_dfs = {}\\n    min_dfs = {}\\n    mean_dfs = {}\\n    sigma_dfs = {}\\n\\n    # Iterate over the new list of variables\\n    for var in new_name_of_variables:\\n        max_dfs[var] = pd.DataFrame()\\n        min_dfs[var] = pd.DataFrame()\\n        mean_dfs[var] = pd.DataFrame()\\n        sigma_dfs[var] = pd.DataFrame()\\n\\n        for storm_num in range(1, 97):\\n            if 'geopotential' in var:\\n                continue  # Skipped to be handled later\\n            else:\\n                file_suffix = '0'\\n                paths = {\\n                    'max': f'{data_folder_path}/{var}/storm_{storm_num}/max_{storm_num}_{file_suffix}.csv',\\n                    'min': f'{data_folder_path}/{var}/storm_{storm_num}/min_{storm_num}_{file_suffix}.csv',\\n                    'mean': f'{data_folder_path}/{var}/storm_{storm_num}/mean_{storm_num}_{file_suffix}.csv',\\n                    'sigma': f'{data_folder_path}/{var}/storm_{storm_num}/std_{storm_num}_{file_suffix}.csv'\\n                }\\n\\n                df_max_temp = pd.read_csv(paths['max']).drop(columns=['Unnamed: 0'])\\n                df_min_temp = pd.read_csv(paths['min']).drop(columns=['Unnamed: 0'])\\n                df_mean_temp = pd.read_csv(paths['mean']).drop(columns=['Unnamed: 0'])\\n                df_sigma_temp = pd.read_csv(paths['sigma']).drop(columns=['Unnamed: 0'])\\n\\n                max_dfs[var][f'storm_{storm_num}'] = df_max_temp['0']\\n                min_dfs[var][f'storm_{storm_num}'] = df_min_temp['0']\\n                mean_dfs[var][f'storm_{storm_num}'] = df_mean_temp['0']\\n                sigma_dfs[var][f'storm_{storm_num}'] = df_sigma_temp['0']\\n\\n    for level in [500, 1000]:\\n        var = f'geopotential_{level}'\\n        max_dfs[var] = pd.DataFrame()\\n        min_dfs[var] = pd.DataFrame()\\n        mean_dfs[var] = pd.DataFrame()\\n        sigma_dfs[var] = pd.DataFrame()\\n\\n        for storm_num in range(1, 97):\\n            paths = {\\n                'max': f'{data_folder_path}/geopotential/storm_{storm_num}/max_{storm_num}_{level}.csv',\\n                'min': f'{data_folder_path}/geopotential/storm_{storm_num}/min_{storm_num}_{level}.csv',\\n                'mean': f'{data_folder_path}/geopotential/storm_{storm_num}/mean_{storm_num}_{level}.csv',\\n                'sigma': f'{data_folder_path}/geopotential/storm_{storm_num}/std_{storm_num}_{level}.csv'\\n            }\\n\\n            df_max_temp = pd.read_csv(paths['max']).drop(columns=['Unnamed: 0'])\\n            df_min_temp = pd.read_csv(paths['min']).drop(columns=['Unnamed: 0'])\\n            df_mean_temp = pd.read_csv(paths['mean']).drop(columns=['Unnamed: 0'])\\n            df_sigma_temp = pd.read_csv(paths['sigma']).drop(columns=['Unnamed: 0'])\\n\\n            max_dfs[var][f'storm_{storm_num}'] = df_max_temp['0']\\n            min_dfs[var][f'storm_{storm_num}'] = df_min_temp['0']\\n            mean_dfs[var][f'storm_{storm_num}'] = df_mean_temp['0']\\n            sigma_dfs[var][f'storm_{storm_num}'] = df_sigma_temp['0']\\n\\n    return max_dfs, min_dfs, mean_dfs, sigma_dfs\\n\\n# Example usage:\\nvariable_list_path = '/Users/fabienaugsburger/Documents/GitHub/master-project/variable_list_24_v2_1.csv'\\ndata_folder_path = '/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h'\\nmax_dfs, min_dfs, mean_dfs, sigma_dfs = process_storm_data(variable_list_path, data_folder_path)\\n\\nname_of_variables = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/variable_list_24_v2_1.csv')\\nname_of_variables = name_of_variables['variable'].tolist()\\n\\nnew_name_of_variables = []\\nfor variable in name_of_variables:\\n        if variable == 'geopotential':\\n            new_name_of_variables.extend(['geopotential_500', 'geopotential_1000'])\\n        else:\\n            new_name_of_variables.append(variable)\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def process_storm_data(variable_list_path, data_folder_path):\n",
    "    # Read the list of variables\n",
    "    name_of_variables = pd.read_csv(variable_list_path)\n",
    "    name_of_variables = name_of_variables['variable'].tolist()\n",
    "\n",
    "    # Create a new list of variables\n",
    "    new_name_of_variables = []\n",
    "    for variable in name_of_variables:\n",
    "        if variable == 'geopotential':\n",
    "            new_name_of_variables.extend(['geopotential_500', 'geopotential_1000'])\n",
    "        else:\n",
    "            new_name_of_variables.append(variable)\n",
    "\n",
    "    # Create dictionaries to hold the DataFrames\n",
    "    max_dfs = {}\n",
    "    min_dfs = {}\n",
    "    mean_dfs = {}\n",
    "    sigma_dfs = {}\n",
    "\n",
    "    # Iterate over the new list of variables\n",
    "    for var in new_name_of_variables:\n",
    "        max_dfs[var] = pd.DataFrame()\n",
    "        min_dfs[var] = pd.DataFrame()\n",
    "        mean_dfs[var] = pd.DataFrame()\n",
    "        sigma_dfs[var] = pd.DataFrame()\n",
    "\n",
    "        for storm_num in range(1, 97):\n",
    "            if 'geopotential' in var:\n",
    "                continue  # Skipped to be handled later\n",
    "            else:\n",
    "                file_suffix = '0'\n",
    "                paths = {\n",
    "                    'max': f'{data_folder_path}/{var}/storm_{storm_num}/max_{storm_num}_{file_suffix}.csv',\n",
    "                    'min': f'{data_folder_path}/{var}/storm_{storm_num}/min_{storm_num}_{file_suffix}.csv',\n",
    "                    'mean': f'{data_folder_path}/{var}/storm_{storm_num}/mean_{storm_num}_{file_suffix}.csv',\n",
    "                    'sigma': f'{data_folder_path}/{var}/storm_{storm_num}/std_{storm_num}_{file_suffix}.csv'\n",
    "                }\n",
    "\n",
    "                df_max_temp = pd.read_csv(paths['max']).drop(columns=['Unnamed: 0'])\n",
    "                df_min_temp = pd.read_csv(paths['min']).drop(columns=['Unnamed: 0'])\n",
    "                df_mean_temp = pd.read_csv(paths['mean']).drop(columns=['Unnamed: 0'])\n",
    "                df_sigma_temp = pd.read_csv(paths['sigma']).drop(columns=['Unnamed: 0'])\n",
    "\n",
    "                max_dfs[var][f'storm_{storm_num}'] = df_max_temp['0']\n",
    "                min_dfs[var][f'storm_{storm_num}'] = df_min_temp['0']\n",
    "                mean_dfs[var][f'storm_{storm_num}'] = df_mean_temp['0']\n",
    "                sigma_dfs[var][f'storm_{storm_num}'] = df_sigma_temp['0']\n",
    "\n",
    "    for level in [500, 1000]:\n",
    "        var = f'geopotential_{level}'\n",
    "        max_dfs[var] = pd.DataFrame()\n",
    "        min_dfs[var] = pd.DataFrame()\n",
    "        mean_dfs[var] = pd.DataFrame()\n",
    "        sigma_dfs[var] = pd.DataFrame()\n",
    "\n",
    "        for storm_num in range(1, 97):\n",
    "            paths = {\n",
    "                'max': f'{data_folder_path}/geopotential/storm_{storm_num}/max_{storm_num}_{level}.csv',\n",
    "                'min': f'{data_folder_path}/geopotential/storm_{storm_num}/min_{storm_num}_{level}.csv',\n",
    "                'mean': f'{data_folder_path}/geopotential/storm_{storm_num}/mean_{storm_num}_{level}.csv',\n",
    "                'sigma': f'{data_folder_path}/geopotential/storm_{storm_num}/std_{storm_num}_{level}.csv'\n",
    "            }\n",
    "\n",
    "            df_max_temp = pd.read_csv(paths['max']).drop(columns=['Unnamed: 0'])\n",
    "            df_min_temp = pd.read_csv(paths['min']).drop(columns=['Unnamed: 0'])\n",
    "            df_mean_temp = pd.read_csv(paths['mean']).drop(columns=['Unnamed: 0'])\n",
    "            df_sigma_temp = pd.read_csv(paths['sigma']).drop(columns=['Unnamed: 0'])\n",
    "\n",
    "            max_dfs[var][f'storm_{storm_num}'] = df_max_temp['0']\n",
    "            min_dfs[var][f'storm_{storm_num}'] = df_min_temp['0']\n",
    "            mean_dfs[var][f'storm_{storm_num}'] = df_mean_temp['0']\n",
    "            sigma_dfs[var][f'storm_{storm_num}'] = df_sigma_temp['0']\n",
    "\n",
    "    return max_dfs, min_dfs, mean_dfs, sigma_dfs\n",
    "\n",
    "# Example usage:\n",
    "variable_list_path = '/Users/fabienaugsburger/Documents/GitHub/master-project/variable_list_24_v2_1.csv'\n",
    "data_folder_path = '/Users/fabienaugsburger/Documents/GitHub/master-project/datasets_3h'\n",
    "max_dfs, min_dfs, mean_dfs, sigma_dfs = process_storm_data(variable_list_path, data_folder_path)\n",
    "\n",
    "name_of_variables = pd.read_csv('/Users/fabienaugsburger/Documents/GitHub/master-project/variable_list_24_v2_1.csv')\n",
    "name_of_variables = name_of_variables['variable'].tolist()\n",
    "\n",
    "new_name_of_variables = []\n",
    "for variable in name_of_variables:\n",
    "        if variable == 'geopotential':\n",
    "            new_name_of_variables.extend(['geopotential_500', 'geopotential_1000'])\n",
    "        else:\n",
    "            new_name_of_variables.append(variable)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# remove variable with only one repetitive value in each statistic\\nfor i in range (0, len(name_of_variables)):\\n    var_name = name_of_variables[i]\\n    for stat in ['max', 'min', 'mean', 'sigma']:\\n        df = locals()[f'{stat}_{var_name}']\\n        cols_to_drop = [col for col in df.columns if df[col].nunique() <= 1]\\n        locals()[f'{stat}_{var_name}'] = df.drop(columns=cols_to_drop)\\n\\n# drop nan values and remove empty dataframes\\nkept_variables = []\\nfor i in range (0, len(name_of_variables)):\\n    var_name = name_of_variables[i]\\n    for stat in ['max', 'min', 'mean', 'sigma']:\\n        df = locals()[f'{stat}_{var_name}']\\n        df = df.dropna()\\n        if not df.empty:\\n            locals()[f'{stat}_{var_name}'] = df\\n            if var_name not in kept_variables:\\n                kept_variables.append(var_name)\\n        else:\\n            del locals()[f'{stat}_{var_name}']\\n\\ndel max_high_cloud_cover\\ndel min_high_cloud_cover\\n\\n# update name_of_variables to only include kept variables\\nname_of_variables = pd.Series(kept_variables)\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "# remove variable with only one repetitive value in each statistic\n",
    "for i in range (0, len(name_of_variables)):\n",
    "    var_name = name_of_variables[i]\n",
    "    for stat in ['max', 'min', 'mean', 'sigma']:\n",
    "        df = locals()[f'{stat}_{var_name}']\n",
    "        cols_to_drop = [col for col in df.columns if df[col].nunique() <= 1]\n",
    "        locals()[f'{stat}_{var_name}'] = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# drop nan values and remove empty dataframes\n",
    "kept_variables = []\n",
    "for i in range (0, len(name_of_variables)):\n",
    "    var_name = name_of_variables[i]\n",
    "    for stat in ['max', 'min', 'mean', 'sigma']:\n",
    "        df = locals()[f'{stat}_{var_name}']\n",
    "        df = df.dropna()\n",
    "        if not df.empty:\n",
    "            locals()[f'{stat}_{var_name}'] = df\n",
    "            if var_name not in kept_variables:\n",
    "                kept_variables.append(var_name)\n",
    "        else:\n",
    "            del locals()[f'{stat}_{var_name}']\n",
    "\n",
    "del max_high_cloud_cover\n",
    "del min_high_cloud_cover\n",
    "\n",
    "# update name_of_variables to only include kept variables\n",
    "name_of_variables = pd.Series(kept_variables)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x_variable = mean_total_precipitation\\n\\n# split the data into a training and a test set\\nX_train, X_test, y_train, y_test = train_test_split(x_variable, target_variable, test_size=0.2, random_state=42)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the target variable is the mean of instanteneous_10m_wind_gust\n",
    "#variable_w_high_corr= ['mean_large_scale_precipitation','mean_total_precipitation','mean_mean_large_scale_precipitation_rate','mean_large_scale_snowfall']\n",
    "#mean_large_scale_precipitation\n",
    "\n",
    "# mean_low_tropospheric_vertical_velocity??\n",
    "# create a dataframe with the target variable\n",
    "\n",
    "#target_variable = mean_instantaneous_10m_wind_gust\n",
    "'''x_variable = mean_total_precipitation\n",
    "\n",
    "# split the data into a training and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_variable, target_variable, test_size=0.2, random_state=42)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"df = mean_large_scale_precipitation\\ndf = df.rename(columns={'0':'mean_large_scale_precipitation'})\\n#df['mean_large_scale_precipitation'] = mean_large_scale_precipitation\\ndf['mean_total_precipitation'] = mean_total_precipitation\\ndf['mean_mean_large_scale_precipitation_rate'] = mean_mean_large_scale_precipitation_rate\\ndf['mean_large_scale_snowfall'] = mean_large_scale_snowfall\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the mean_large_scale_precipitation in a df, set the name as the header of the column and so the same with mean_total_precipitation\n",
    "import random\n",
    "\n",
    "def setup_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)                       \n",
    "setup_seed(42)\n",
    "\n",
    "number_test_storms = round(96*0.3)\n",
    "test_storm_index = random.sample(range(96), number_test_storms)\n",
    "number_validation_storms = round(number_test_storms*1/3)\n",
    "validation_storm_index = random.sample(test_storm_index, number_validation_storms)\n",
    "\n",
    "# order the index of the test_storm_index and validation_storm_index\n",
    "test_storm_index.sort()\n",
    "validation_storm_index.sort()\n",
    "\n",
    "# remove the index of the validation storms from the test_storm_index\n",
    "'''for i in range(0, len(validation_storm_index)):\n",
    "    test_storm_index.remove(validation_storm_index[i])'''\n",
    "\n",
    "# check the number of storms in each set\n",
    "print(len(test_storm_index))\n",
    "print(len(validation_storm_index))\n",
    "\n",
    "# create the training, validation and test set\n",
    "X_train = pd.DataFrame()\n",
    "X_validation = pd.DataFrame()\n",
    "X_test = pd.DataFrame()\n",
    "y_train = pd.DataFrame()\n",
    "y_validation = pd.DataFrame()\n",
    "y_test = pd.DataFrame()\n",
    "\n",
    "if 'instantaneous_10m_wind_gust' in new_name_of_variables:\n",
    "    new_name_of_variables.remove('instantaneous_10m_wind_gust')\n",
    "\n",
    "for x_var in new_name_of_variables: \n",
    "    for storm_number in locals()[f'mean_{x_var}'].columns:\n",
    "        modified_storm_number = f\"{x_var}_{storm_number}\"\n",
    "        if int(storm_number.split('_')[1]) in test_storm_index:\n",
    "            if int(storm_number.split('_')[1]) in validation_storm_index:\n",
    "                X_validation = pd.concat([X_validation, locals()[f'mean_{x_var}'][storm_number]], axis=1) # [storm_number]\n",
    "                X_validation = X_validation.rename(columns={storm_number: modified_storm_number})\n",
    "                #y_validation = pd.concat([y_validation, mean_instantaneous_10m_wind_gust[storm_number]], axis=1)\n",
    "            else:\n",
    "                X_test = pd.concat([X_test, locals()[f'mean_{x_var}'][storm_number]], axis=1)\n",
    "                X_test = X_test.rename(columns={storm_number: modified_storm_number})\n",
    "                #y_test = pd.concat([y_test, locals()[f'mean_instantaneous_10m_wind_gust'][storm_number]], axis=1)\n",
    "        else:\n",
    "            X_train = pd.concat([X_train, locals()[f'mean_{x_var}'][storm_number]], axis=1)\n",
    "            X_train = X_train.rename(columns={storm_number: modified_storm_number})\n",
    "            #y_train = pd.concat([y_train, locals()[f'mean_instantaneous_10m_wind_gust'][storm_number]], axis=1)\n",
    "\n",
    "for storm_number in mean_instantaneous_10m_wind_gust.columns:\n",
    "    modified_storm_number = f\"mean_instantaneous_10m_wind_gust_{storm_number}\"\n",
    "    if int(storm_number.split('_')[1]) in test_storm_index:\n",
    "        if int(storm_number.split('_')[1]) in validation_storm_index:\n",
    "            y_validation = pd.concat([y_validation, mean_instantaneous_10m_wind_gust[storm_number]], axis=1)\n",
    "            y_validation = y_validation.rename(columns={storm_number: modified_storm_number})\n",
    "        else:\n",
    "            y_test = pd.concat([y_test, mean_instantaneous_10m_wind_gust[storm_number]], axis=1)\n",
    "            y_test = y_test.rename(columns={storm_number: modified_storm_number})\n",
    "    else:\n",
    "        y_train = pd.concat([y_train, mean_instantaneous_10m_wind_gust[storm_number]], axis=1)\n",
    "        y_train = y_train.rename(columns={storm_number: modified_storm_number})\n",
    "\n",
    "\n",
    "'''df = mean_large_scale_precipitation\n",
    "df = df.rename(columns={'0':'mean_large_scale_precipitation'})\n",
    "#df['mean_large_scale_precipitation'] = mean_large_scale_precipitation\n",
    "df['mean_total_precipitation'] = mean_total_precipitation\n",
    "df['mean_mean_large_scale_precipitation_rate'] = mean_mean_large_scale_precipitation_rate\n",
    "df['mean_large_scale_snowfall'] = mean_large_scale_snowfall\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'combined_df = combine_columns(df)\\nprint(combined_df)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_columns(df):\n",
    "    # Extract variable names from column names\n",
    "    columns_info = [col.rsplit('_', 2) for col in df.columns]\n",
    "    \n",
    "    # Create a dictionary to hold grouped columns\n",
    "    grouped_columns = {}\n",
    "    for var, unit, storm in columns_info:\n",
    "        variable_name = f\"{var}\"#_{unit}\"\n",
    "        if variable_name not in grouped_columns:\n",
    "            grouped_columns[variable_name] = []\n",
    "        grouped_columns[variable_name].append(f\"{var}_{unit}_{storm}\")\n",
    "    \n",
    "    # Create a new dataframe to hold the combined columns\n",
    "    combined_df = pd.DataFrame()\n",
    "    \n",
    "    for variable, cols in grouped_columns.items():\n",
    "        # Combine the columns for each variable\n",
    "        combined_variable_df = pd.concat([df[col] for col in cols], ignore_index=True) #.dropna()\n",
    "        combined_df[variable] = combined_variable_df\n",
    "        #combined_df = combined_df.dropna(axis=0, how='any')\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Example usage\n",
    "'''data = {\n",
    "    '2m_dewpoint_temperature_storm_1': [1, 2, None, 4],\n",
    "    '2m_dewpoint_temperature_storm_2': [None, 5, 6, None],\n",
    "    '2m_dewpoint_temperature_storm_3': [7, None, 9, 10],\n",
    "    'other_variable_storm_1': [10, 20, 30, 40],\n",
    "    'other_variable_storm_2': [None, 25, 35, None],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)'''\n",
    "'''combined_df = combine_columns(df)\n",
    "print(combined_df)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_variable = mean_instantaneous_10m_wind_gust\n",
    "\n",
    "\n",
    "'''X_train_reshape = X_train.values.reshape((X_train.shape[0], X_train.shape[1]))\n",
    "X_test_reshape = X_test.values.reshape((X_test.shape[0], X_test.shape[1]))\n",
    "y_train_reshape = y_train.values.reshape((y_train.shape[0], y_train.shape[1]))\n",
    "y_test_reshape = y_test.values.reshape((y_test.shape[0], y_test.shape[1]))'''\n",
    "\n",
    "# reshape all X and y such as the shape is (all the storms, all the variables)\n",
    "# Merge the columns\n",
    "\n",
    "X_train_new= combine_columns(X_train)\n",
    "X_test_new= combine_columns(X_test)\n",
    "X_validation_new= combine_columns(X_validation)\n",
    "\n",
    "y_train_new= combine_columns(y_train)\n",
    "y_test_new= combine_columns(y_test)\n",
    "y_validation_new= combine_columns(y_validation)\n",
    "\n",
    "# drop the convective inhibition and cloud_base_height\n",
    "X_train_new = X_train_new.drop(columns = ['convective_inhibition','cloud_base_height'])\n",
    "X_test_new = X_test_new.drop(columns = ['convective_inhibition','cloud_base_height'])\n",
    "X_validation_new = X_validation_new.drop(columns = ['convective_inhibition','cloud_base_height'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"X_train_new['target'] = y_train_new\\nX_train_new = X_train_new.dropna()\\ny_train_new = X_train_new['target']\\nX_train_new = X_train_new.drop(columns = ['target'])\\n\\nX_test_new['target'] = y_test_new\\nX_test_new = X_test_new.dropna()\\ny_test_new = X_test_new['target']\\nX_test_new = X_test_new.drop(columns = ['target'])\\n\\nX_validation_new['target'] = y_validation_new\\nX_validation_new = X_validation_new.dropna()\\ny_validation_new = X_validation_new['target']\\nX_validation_new = X_validation_new.drop(columns = ['target'])\\n\\n# check the shape of the new X and y\\nprint(X_train_new.shape, y_train_new.shape)\\nprint(X_test_new.shape, y_test_new.shape)\\nprint(X_validation_new.shape, y_validation_new.shape)\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop nan values by merging the X and y\n",
    "\n",
    "def drop_nan(X, y):\n",
    "    X['target'] = y\n",
    "    X = X.dropna()\n",
    "    y = X['target']\n",
    "    X = X.drop(columns = ['target'])\n",
    "    return X, y\n",
    "\n",
    "X_train_new, y_train_new = drop_nan(X_train_new, y_train_new)\n",
    "X_test_new, y_test_new = drop_nan(X_test_new, y_test_new)\n",
    "X_validation_new, y_validation_new = drop_nan(X_validation_new, y_validation_new)\n",
    "\n",
    "'''X_train_new['target'] = y_train_new\n",
    "X_train_new = X_train_new.dropna()\n",
    "y_train_new = X_train_new['target']\n",
    "X_train_new = X_train_new.drop(columns = ['target'])\n",
    "\n",
    "X_test_new['target'] = y_test_new\n",
    "X_test_new = X_test_new.dropna()\n",
    "y_test_new = X_test_new['target']\n",
    "X_test_new = X_test_new.drop(columns = ['target'])\n",
    "\n",
    "X_validation_new['target'] = y_validation_new\n",
    "X_validation_new = X_validation_new.dropna()\n",
    "y_validation_new = X_validation_new['target']\n",
    "X_validation_new = X_validation_new.drop(columns = ['target'])\n",
    "\n",
    "# check the shape of the new X and y\n",
    "print(X_train_new.shape, y_train_new.shape)\n",
    "print(X_test_new.shape, y_test_new.shape)\n",
    "print(X_validation_new.shape, y_validation_new.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./LSTM_basic_logs/run_005\n"
     ]
    }
   ],
   "source": [
    "run_index = 5 # it should be an integer, e.g. 1\n",
    "\n",
    "run_logdir = os.path.join(os.curdir, \"LSTM_basic_logs/\", \"run_{:03d}\".format(run_index))\n",
    "\n",
    "print(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks (they can really improve the accuracy if well-chosen!)\n",
    "import tensorflow as tf\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=30)\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"model.h5\", \n",
    "                                                   save_best_only=True,\n",
    "                                                   monitor='loss')\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 18:23:07.316918: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 6ms/step - loss: 3260294.2500 - val_loss: 186163.0312\n",
      "Epoch 2/156\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 19381.1191 - val_loss: 96.2705\n",
      "Epoch 3/156\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 107.4200 - val_loss: 87.9632\n",
      "Epoch 4/156\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 105.6560 - val_loss: 86.6082\n",
      "Epoch 5/156\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 105.4712 - val_loss: 88.8137\n",
      "Epoch 6/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 105.7394 - val_loss: 87.8484\n",
      "Epoch 7/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 105.0366 - val_loss: 84.1880\n",
      "Epoch 8/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 104.0252 - val_loss: 85.9640\n",
      "Epoch 9/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 103.4155 - val_loss: 85.6142\n",
      "Epoch 10/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 102.5816 - val_loss: 85.5883\n",
      "Epoch 11/156\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 102.2244 - val_loss: 86.2861\n",
      "Epoch 12/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 102.3821 - val_loss: 84.3009\n",
      "Epoch 13/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 103.4171 - val_loss: 92.6150\n",
      "Epoch 14/156\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 115.7851 - val_loss: 92.4427\n",
      "Epoch 15/156\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 115.2812 - val_loss: 93.4397\n",
      "Epoch 16/156\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 114.3091 - val_loss: 92.5007\n",
      "Epoch 17/156\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 113.6153 - val_loss: 90.2773\n",
      "Epoch 18/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 112.9503 - val_loss: 93.6168\n",
      "Epoch 19/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 112.6686 - val_loss: 94.5592\n",
      "Epoch 20/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 111.8740 - val_loss: 94.3558\n",
      "Epoch 21/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 110.7739 - val_loss: 88.4209\n",
      "Epoch 22/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 108.9386 - val_loss: 89.0814\n",
      "Epoch 23/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 107.1597 - val_loss: 86.8865\n",
      "Epoch 24/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 105.6978 - val_loss: 92.3538\n",
      "Epoch 25/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 105.2727 - val_loss: 82.5327\n",
      "Epoch 26/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 103.9714 - val_loss: 82.5287\n",
      "Epoch 27/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 132.5795 - val_loss: 133.0467\n",
      "Epoch 28/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 112.7954 - val_loss: 90.3991\n",
      "Epoch 29/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 107.7698 - val_loss: 87.5398\n",
      "Epoch 30/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 107.7067 - val_loss: 89.2508\n",
      "Epoch 31/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 108.8015 - val_loss: 87.7312\n",
      "Epoch 32/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 107.4451 - val_loss: 86.1495\n",
      "Epoch 33/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 105.2108 - val_loss: 85.5220\n",
      "Epoch 34/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 105.6723 - val_loss: 86.7384\n",
      "Epoch 35/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 105.1143 - val_loss: 87.2707\n",
      "Epoch 36/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 102.5605 - val_loss: 84.4707\n",
      "Epoch 37/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 99.4918 - val_loss: 83.9473\n",
      "Epoch 38/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 95.0444 - val_loss: 80.7202\n",
      "Epoch 39/156\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 159.5584 - val_loss: 79.2888\n",
      "Epoch 40/156\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 5094323712.0000 - val_loss: 5460842.5000\n",
      "Epoch 41/156\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 1915812.6250 - val_loss: 6067.0229\n",
      "Epoch 42/156\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 4919.5532 - val_loss: 3993.3550\n",
      "Epoch 43/156\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 3182.1411 - val_loss: 1372.9417\n",
      "Epoch 44/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 1016.7697 - val_loss: 1253.4532\n",
      "Epoch 45/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 942.8659 - val_loss: 1245.2955\n",
      "Epoch 46/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 944.5571 - val_loss: 1241.2052\n",
      "Epoch 47/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 943.1215 - val_loss: 1230.9374\n",
      "Epoch 48/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 935.1663 - val_loss: 1247.5245\n",
      "Epoch 49/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 940.5150 - val_loss: 1214.7523\n",
      "Epoch 50/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 941.9537 - val_loss: 1201.0645\n",
      "Epoch 51/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 830.6362 - val_loss: 943.2822\n",
      "Epoch 52/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 583.3168 - val_loss: 875.1549\n",
      "Epoch 53/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 539.7508 - val_loss: 846.7897\n",
      "Epoch 54/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 505.0817 - val_loss: 795.4618\n",
      "Epoch 55/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 482.0625 - val_loss: 742.3938\n",
      "Epoch 56/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 462.0992 - val_loss: 659.8091\n",
      "Epoch 57/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 450.0508 - val_loss: 652.9859\n",
      "Epoch 58/156\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 417.7853 - val_loss: 625.2673\n",
      "Epoch 59/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 408.4485 - val_loss: 547.6119\n",
      "Epoch 60/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 401.9749 - val_loss: 488.3905\n",
      "Epoch 61/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 396.4505 - val_loss: 1115.3646\n",
      "Epoch 62/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 454.9190 - val_loss: 1052.3398\n",
      "Epoch 63/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 459.1465 - val_loss: 1041.2822\n",
      "Epoch 64/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 440.5500 - val_loss: 1028.5648\n",
      "Epoch 65/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 418.9902 - val_loss: 360.4864\n",
      "Epoch 66/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 367.6538 - val_loss: 453.9971\n",
      "Epoch 67/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 355.9538 - val_loss: 989.8312\n",
      "Epoch 68/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 358.7689 - val_loss: 319.8132\n",
      "Epoch 69/156\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 344.4591 - val_loss: 905.3889\n",
      "26/26 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Définir le modèle\n",
    "'''X = df #pd.concat([locals()[f'{x_var}']['0'] for x_var in variable_w_high_corr if f'{x_var}' in locals()], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target_variable, test_size=0.2, random_state=42)\n",
    "'''\n",
    "model = Sequential()\n",
    "# Ajouter le nombre de variables à haute corrélation comme variable indépendante\n",
    "model.add(LSTM(50, activation='relu', input_shape=(X_train_new.shape[1], 1)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Diviser les données en ensembles de formation et de test\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, target_variable, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Redimensionner les données pour LSTM\n",
    "'''if len(X_train.shape) == 2:\n",
    "    X_train = X_train.values.reshape((X_train.shape[0], X_train.shape[1]))\n",
    "    X_test = X_test.values.reshape((X_test.shape[0], X_test.shape[1]))\n",
    "    y_train = y_train.values.reshape((y_train.shape[0], y_train.shape[1]))\n",
    "    y_test = y_test.values.reshape((y_test.shape[0], y_test.shape[1]))'''\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.fit(X_train_new, y_train_new, \n",
    "          epochs=156, \n",
    "          verbose=1, \n",
    "          use_multiprocessing=True, \n",
    "          validation_data=(X_validation_new, y_validation_new), \n",
    "          callbacks=[early_stopping_cb, checkpoint_cb, tensorboard_cb])\n",
    "\n",
    "# Faire des prédictions\n",
    "y_pred = model.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir=./LSTM_basic_logs --port=1234\n",
    "# to launch tensorboard in the terminal\n",
    "#tensorboard --logdir=/Users/fabienaugsburger/Documents/GitHub/master-project/case_study/LSTM_basic_logs --port=1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [01:43<04:24,  3.72s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau s’est bloqué lors de l’exécution du code dans une cellule active ou une cellule précédente. \n",
      "\u001b[1;31mVeuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. \n",
      "\u001b[1;31mCliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. \n",
      "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "# use shapely to calculate the shap values\n",
    "\n",
    "import shap\n",
    "\n",
    "# Sample a subset of the data\n",
    "sample_size = 100  # Adjust this to a size your machine can handle\n",
    "X_sample = X_test_new.sample(sample_size, random_state=42)\n",
    "\n",
    "# see the explanation for the model's predictions using SHAP\n",
    "#explainer = shap.DeepExplainer(model, X_test_new)\n",
    "explainer = shap.KernelExplainer(model, X_sample)\n",
    "shap_values = explainer(X_sample)\n",
    "\n",
    "#shap.summary_plot(shap_values, X_test_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the shap values for the first prediction\n",
    "print(np.asarray(shap_values[:].values).shape)\n",
    "test = np.squeeze(np.asarray(shap_values[:].values))\n",
    "test_m = np.mean(test, axis=0)\n",
    "#shap.plots.waterfall(shap_values[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.squeeze(y_test), label='True', lw=0, marker='o')\n",
    "#plt.plot(y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance \n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(model, X_train, y_train, n_repeats=10, random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer l'erreur quadratique moyenne\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"L'erreur quadratique moyenne du modèle avec {variable_w_high_corr} comme variable indépendante est {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"L'erreur absolue moyenne (MAE) du modèle est {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "rmse = sqrt(mse) # mse est l'erreur quadratique moyenne calculée précédemment\n",
    "print(f\"La racine de l'erreur quadratique moyenne (RMSE) du modèle est {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Le coefficient de détermination (R²) du modèle est {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
