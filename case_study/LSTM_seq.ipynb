{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/01/5ryz4pnn581dj9gk6r1nn5qr0000gn/T/ipykernel_23782/1367394781.py:27: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  data['temperature'].iloc[950:] = 0\n",
      "/var/folders/01/5ryz4pnn581dj9gk6r1nn5qr0000gn/T/ipykernel_23782/1367394781.py:28: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  data['humidity'].iloc[950:] = 0\n",
      "/var/folders/01/5ryz4pnn581dj9gk6r1nn5qr0000gn/T/ipykernel_23782/1367394781.py:29: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  data['pressure'].iloc[950:] = 0\n",
      "/Users/fabienaugsburger/anaconda3/envs/tensor_flow/lib/python3.11/site-packages/keras/src/layers/core/masking.py:47: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 - 1s - 71ms/step - loss: 69.7318 - val_loss: 75.7387\n",
      "Epoch 2/10\n",
      "20/20 - 0s - 16ms/step - loss: 68.5255 - val_loss: 74.4312\n",
      "Epoch 3/10\n",
      "20/20 - 0s - 15ms/step - loss: 67.2431 - val_loss: 72.9512\n",
      "Epoch 4/10\n",
      "20/20 - 0s - 15ms/step - loss: 65.7398 - val_loss: 71.2187\n",
      "Epoch 5/10\n",
      "20/20 - 0s - 15ms/step - loss: 63.9191 - val_loss: 69.0960\n",
      "Epoch 6/10\n",
      "20/20 - 0s - 15ms/step - loss: 61.7117 - val_loss: 66.4935\n",
      "Epoch 7/10\n",
      "20/20 - 0s - 15ms/step - loss: 59.0776 - val_loss: 63.3985\n",
      "Epoch 8/10\n",
      "20/20 - 0s - 15ms/step - loss: 55.9986 - val_loss: 59.8827\n",
      "Epoch 9/10\n",
      "20/20 - 0s - 16ms/step - loss: 52.5743 - val_loss: 55.8989\n",
      "Epoch 10/10\n",
      "20/20 - 0s - 15ms/step - loss: 48.7868 - val_loss: 51.6246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 800 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "ename": "DimensionError",
     "evalue": "Instance must have 1 or 2 dimensions!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDimensionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Create a SHAP explainer for the LSTM model\u001b[39;00m\n\u001b[1;32m     58\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mKernelExplainer(model, X_train)\n\u001b[0;32m---> 59\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(X_test)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Since we are predicting a single output, we use shap_values[0]\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# SHAP summary plot\u001b[39;00m\n\u001b[1;32m     63\u001b[0m shap\u001b[38;5;241m.\u001b[39msummary_plot(shap_values[\u001b[38;5;241m0\u001b[39m], X_test, feature_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhumidity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpressure\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/tensor_flow/lib/python3.11/site-packages/shap/explainers/_kernel.py:294\u001b[0m, in \u001b[0;36mKernelExplainer.shap_values\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     emsg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstance must have 1 or 2 dimensions!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DimensionError(emsg)\n",
      "\u001b[0;31mDimensionError\u001b[0m: Instance must have 1 or 2 dimensions!"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate random data\n",
    "np.random.seed(42)\n",
    "num_samples = 1000\n",
    "temperature = np.random.rand(num_samples) * 40  # Temperature in Celsius\n",
    "humidity = np.random.rand(num_samples) * 100  # Humidity in percentage\n",
    "pressure = np.random.rand(num_samples) * 20 + 980  # Pressure in hPa\n",
    "wind_speed = np.random.rand(num_samples) * 15  # Wind speed in m/s\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'temperature': temperature,\n",
    "    'humidity': humidity,\n",
    "    'pressure': pressure,\n",
    "    'wind_speed': wind_speed\n",
    "})\n",
    "\n",
    "# Introduce padding\n",
    "data['temperature'].iloc[950:] = 0\n",
    "data['humidity'].iloc[950:] = 0\n",
    "data['pressure'].iloc[950:] = 0\n",
    "\n",
    "# Split into features and target\n",
    "X = data[['temperature', 'humidity', 'pressure']].values\n",
    "y = data['wind_speed'].values\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape X to be 3D for LSTM (samples, timesteps, features)\n",
    "X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0, input_shape=(1, X_train.shape[2])))  # Masking layer\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=2)\n",
    "\n",
    "# Create a SHAP explainer for the LSTM model\n",
    "explainer = shap.KernelExplainer(model, X_train)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Since we are predicting a single output, we use shap_values[0]\n",
    "# SHAP summary plot\n",
    "shap.summary_plot(shap_values[0], X_test, feature_names=['temperature', 'humidity', 'pressure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - loss: 0.3350\n",
      "Epoch 2/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1500\n",
      "Epoch 3/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1364\n",
      "Epoch 4/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1145\n",
      "Epoch 5/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1004\n",
      "Epoch 6/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0894\n",
      "Epoch 7/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1052\n",
      "Epoch 8/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0886\n",
      "Epoch 9/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0984\n",
      "Epoch 10/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0767\n",
      "Epoch 11/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0876\n",
      "Epoch 12/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0866\n",
      "Epoch 13/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0868\n",
      "Epoch 14/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0958\n",
      "Epoch 15/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0894\n",
      "Epoch 16/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0969\n",
      "Epoch 17/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0924\n",
      "Epoch 18/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0809\n",
      "Epoch 19/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0766\n",
      "Epoch 20/20\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0824\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 739ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Masking, TimeDistributed\n",
    "\n",
    "# Sample data (dummy data for illustration)\n",
    "# Assume 10 samples, each with 12 hours of historical data (12 time steps), and 3 features\n",
    "historical_data = np.random.rand(10, 12, 3).astype(np.float32)\n",
    "# Corresponding future wind speeds for the next 11 hours\n",
    "future_wind_speeds = np.random.rand(10, 11, 1).astype(np.float32)\n",
    "\n",
    "# Define the model\n",
    "input_seq = Input(shape=(12, 3))\n",
    "masked_input = Masking(mask_value=-1)(input_seq)\n",
    "encoder = LSTM(64, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(masked_input)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder\n",
    "decoder_input = Input(shape=(11, 1))  # Shifted right future wind speed\n",
    "masked_decoder_input = Masking(mask_value=-1)(decoder_input)\n",
    "decoder_lstm = LSTM(64, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(masked_decoder_input, initial_state=encoder_states)\n",
    "decoder_dense = TimeDistributed(Dense(1))  # Output layer\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([input_seq, decoder_input], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "# Shift future_wind_speeds to the right by one time step as decoder input\n",
    "decoder_input_data = np.zeros_like(future_wind_speeds)\n",
    "decoder_input_data[:, 1:, :] = future_wind_speeds[:, :-1, :]\n",
    "decoder_input_data[:, 0, :] = 0  # Assume start of sequence is 0 or another appropriate value\n",
    "\n",
    "model.fit([historical_data, decoder_input_data], future_wind_speeds, epochs=20, batch_size=1)\n",
    "\n",
    "# Evaluate and predict\n",
    "predicted_wind_speeds = model.predict([historical_data, decoder_input_data])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
